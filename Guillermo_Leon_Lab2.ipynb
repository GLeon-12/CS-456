{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb648f04-43aa-4da8-a404-486be06dcce6",
   "metadata": {},
   "source": [
    "Title Section: Data Reading and Processing, Members: Guillermo Leon, Submission Date: 10/13/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336370d-9c4e-4220-b7df-bcabd7f9e4ec",
   "metadata": {},
   "source": [
    "Introduction Section: This program aims to read various data types, identify and rectify issues in the 'damaged' files, which may include missing values, inconsistencies, and format-related problems. The clean information will then be combined from all three data types to create a cohesive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca544b2-8a46-43de-bdcb-f5e8cb979c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57c2a3-1180-4ec3-811c-2ad337656ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883979c-e37e-411e-b1bc-d90acc488d3e",
   "metadata": {},
   "source": [
    "Handle damaged csv data: Within this block of code I decide to use a function .read_csv while skipping bad lines. This allowed me to access the file without running into errors, from this point I decided to rename the columns to their standardized names. I then cleaned and converted the data types for the key columns, removing commas and coercing invalid values to NaN. Finally, I identified all rows with any missing or invalid values across all columns and saved them into a seperate dataframe called bad_rows_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a71f069-c30a-4b50-aec9-fb623ad9965b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file while skipping bad lines\n",
    "df_csv = pd.read_csv('fortune500.csv', on_bad_lines='skip')  \n",
    "\n",
    "# Creating a dictionary that will map the new column names\n",
    "column_dictionary = { \n",
    "    '#Year': 'Year', \n",
    "    '(1)Rank': 'Rank', \n",
    "    '!Company': 'Company', \n",
    "    '(3)Revenue (in millions)': 'Revenue (in millions)', \n",
    "    'okjb)Profit (in millions)': 'Profit (in millions)'\n",
    "} \n",
    "\n",
    "# Renaming columns in the DataFrame using names from the column_dictionary\n",
    "df_csv.rename(columns=column_dictionary, inplace=True) \n",
    "\n",
    "# Making sure each column is in the correct data type by onverting data types with error coercion\n",
    "# If conversion fails coercion will return NaN \n",
    "# Also removing any commas from Revenue and Profit to prevent any formating issues\n",
    "df_csv['Year'] = pd.to_numeric(df_csv['Year'], errors='coerce').astype('Int64')  \n",
    "df_csv['Rank'] = pd.to_numeric(df_csv['Rank'], errors='coerce').astype('Int64') \n",
    "df_csv['Revenue (in millions)'] = pd.to_numeric(df_csv['Revenue (in millions)'].str.replace(',', ''), errors='coerce') \n",
    "df_csv['Profit (in millions)'] = pd.to_numeric(df_csv['Profit (in millions)'].str.replace(',', ''), errors='coerce') \n",
    "\n",
    "# Check all columns for any missing or invalid data and store into bad_rows_df\n",
    "bad_rows_df = df_csv[df_csv.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9013d-e11e-43cc-a41c-a87c6751205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the clean dataframe to see if any issues arised\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a72bb-0b2c-478f-9dc5-896b5fef92a3",
   "metadata": {},
   "source": [
    "Handle damaged json Data: Within this block of code I read the JSON file line by line, ensuring each record had five fields by filling the missing keys with None, and loaded the data into a dataframe. I then cleaned the data by converting each column to the correct data type, handling any invalid entries using coerce, which would turn them into NaN. Finally I seprated the rows with missing or invalid values into df_json_bad and kept only the clean rows in df_json_clean, and then reseting the index for both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b8079d-14b7-48ef-aa25-2a1a56f84541",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"lines.json\"\n",
    "data = []\n",
    "title = [\"Year\", \"Rank\", \"Company\", \"Revenue (in millions)\", \"Profit (in millions)\"]\n",
    "\n",
    "# Read lines.json line by line and parse JSON\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            #parse the JSON string\n",
    "            json_obj = json.loads(line)\n",
    "            \n",
    "            # Ensure all five expected keys (columns) exist, fill with None value if missing\n",
    "            for key in title:\n",
    "                if key not in json_obj:\n",
    "                    json_obj[key] = None\n",
    "\n",
    "            #append the processed JSON object to the list\n",
    "            data.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        \n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df_json = pd.DataFrame(data)\n",
    "\n",
    "# Convert types with coercion, assign values to NaN if errors occur\n",
    "df_json['Year'] = pd.to_numeric(df_json['Year'], errors='coerce').astype('Int64')\n",
    "df_json['Rank'] = pd.to_numeric(df_json['Rank'], errors='coerce').astype('Int64')\n",
    "df_json['Revenue (in millions)'] = pd.to_numeric(df_json['Revenue (in millions)'], errors='coerce')\n",
    "df_json['Profit (in millions)'] = pd.to_numeric(df_json['Profit (in millions)'], errors='coerce')\n",
    "df_json['Company'] = df_json['Company'].astype('string').str.strip()\n",
    "\n",
    "# Creating dictionary to plug into isnull() to find bad rows\n",
    "critical_cols = ['Year', 'Rank', 'Company', 'Revenue (in millions)', 'Profit (in millions)']\n",
    "df_json_bad = df_json[df_json[critical_cols].isnull().any(axis=1)].copy()\n",
    "\n",
    "# Dropping all the bad rows from the df_json and reseting index\n",
    "df_json_clean = df_json.drop(df_json_bad.index).reset_index(drop=True)\n",
    "df_json_bad.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e28d1-b437-402b-8f32-214905632178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing to clean dataframe to see if any issues arise\n",
    "df_json_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a69f9-955a-4793-8892-a3c5fe4b9f62",
   "metadata": {},
   "source": [
    "Handle damaged txt file: This block of code reads an unstructured text file where each record is a block of key-values pairs seperated by blank lines, and it converts each block into a dictionary using the process_block() function. It then attempts to cast each value to its correct data type, if its succesfull then they will be stored in good_rows, if not they will be stored in bad_rows. Finally, both list are converted into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37af89-a467-4fdd-aaba-3ff176964705",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'unstructureddata.txt'\n",
    "\n",
    "# Using the dictionary to map column names to their data types\n",
    "expected_types = {\n",
    "    \"Year\": int,\n",
    "    \"Rank\": int,\n",
    "    \"Company\": str,\n",
    "    \"Revenue (in millions)\": float,\n",
    "    \"Profit (in millions)\": float,\n",
    "}\n",
    "\n",
    "# Function to convert block of key-value lines into a dictionary\n",
    "def process_block(block):\n",
    "    entity = {}\n",
    "    for item in block:\n",
    "            key, value = item.split(':')\n",
    "            entity[key] = value\n",
    "    return entity\n",
    "\n",
    "# creating lists to store values\n",
    "good_rows = []\n",
    "bad_rows = []\n",
    "current_data = []\n",
    "\n",
    "# read and process the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        #if line is blank, signals the end of a block\n",
    "        if not line:\n",
    "            if current_data:\n",
    "                # Converts the list of strings into a dictionary\n",
    "                dfcontent = process_block(current_data)\n",
    "                # Try block will attempt to convert values to their mapped data types\n",
    "                # If conversion succeeds then append the row to the good_rows list\n",
    "                try:\n",
    "                    typed_dfcontent = {\n",
    "                        key: expected_types[key](dfcontent[key]) for key in expected_types\n",
    "                    }\n",
    "                    good_rows.append(typed_dfcontent)\n",
    "                # If error occurs than append that row to the bad_rows list\n",
    "                except Exception:\n",
    "                    bad_rows.append(dfcontent)\n",
    "                    current_data = []\n",
    "        else:\n",
    "            current_data.append(line)\n",
    "\n",
    "# Handle the final dfcontent (if no blank line at end)\n",
    "# checks if any remaining unprocessed dfcontent is still stored in current_data\n",
    "if current_data:\n",
    "    dfcontent = process_block(current_data)\n",
    "\n",
    "    # Try block to attempt type conversion of the values if succeeds then place row in good_rows\n",
    "    # If fails then place bad row in bad_rows\n",
    "    try:\n",
    "        typed_dfcontent = {\n",
    "            key: expected_types[key](dfcontent[key]) for key in expected_types\n",
    "        }\n",
    "        good_rows.append(typed_dfcontent)\n",
    "    except Exception:\n",
    "        bad_rows.append(dfcontent)\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_good = pd.DataFrame(good_rows)\n",
    "df_bad = pd.DataFrame(bad_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adf1bc3-27cf-47b9-93bc-42b9246ca62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the clean dataframe to see if any errors arise\n",
    "df_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff11b04-ab8d-40de-91d0-561799b206c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all three clean dataframes into one dataframe and then printing it\n",
    "combined_df = pd.concat([df_csv, df_json_clean, df_good], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b44ba-4e00-48fc-97b7-2afdb7ffcc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45096781-0a29-4483-a6ac-be9339fc21b4",
   "metadata": {},
   "source": [
    "Data Statistics: This code block calculates key summary statistics from a combined dataset, including the total count of good data records and the total count of bad data rows aggregated from the three sources. It identifies the number of unique companies and finds the companies with the highest revenue and profit between 1995 and 1998. Finally, it organizes all these results into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad8a35-af0c-4bdd-9183-dc66b31d770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data volume (good data count)\n",
    "total_good_data = len(combined_df)\n",
    "\n",
    "# Instances of missing data (bad data count)\n",
    "missing_data_count = len(pd.concat([bad_rows_df, df_json_bad, df_bad], ignore_index=True, sort=False))\n",
    "\n",
    "# Number of unique companies\n",
    "unique_companies = combined_df['Company'].nunique()\n",
    "\n",
    "# Company with highest revenue and highest profit between 1995 and 1998\n",
    "filtered_df = combined_df[(combined_df['Year'] >= 1995) & (combined_df['Year'] <= 1998)]\n",
    "\n",
    "max_revenue = filtered_df['Revenue (in millions)'].max()\n",
    "highest_revenue_rows = filtered_df[filtered_df['Revenue (in millions)'] == max_revenue]\n",
    "# Pick the first company with the highest revenue\n",
    "company_highest_revenue = highest_revenue_rows.iloc[0]['Company']\n",
    "\n",
    "max_profit = filtered_df['Profit (in millions)'].max()\n",
    "# Now filter the rows where Profit equals max_profit (in case of ties)\n",
    "highest_profit_rows = filtered_df[filtered_df['Profit (in millions)'] == max_profit]\n",
    "# If you want just the first one (or you can handle multiple)\n",
    "company_highest_profit = highest_profit_rows.iloc[0]['Company']\n",
    "\n",
    "# Create a dictionary with all results\n",
    "results_dict = {\n",
    "    'Data Statistics:': [\n",
    "        'Aggregate Data Volume (Good Data)',\n",
    "        'Instances of Missing Data (Bad Data)',\n",
    "        'Number of Unique Companies',\n",
    "        'Company with Highest Revenue (1995-1998)',\n",
    "        'Highest Revenue (1995-1998)',\n",
    "        'Company with Highest Profit (1995-1998)',\n",
    "        'Highest Profit (1995-1998)'\n",
    "    ],\n",
    "    '': [\n",
    "        total_good_data,\n",
    "        missing_data_count,\n",
    "        unique_companies,\n",
    "        company_highest_revenue,\n",
    "        max_revenue,\n",
    "        company_highest_profit,\n",
    "        max_profit\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert dictionary to DataFrame and set Data Statistics as index\n",
    "Results_Combine = pd.DataFrame(results_dict).set_index('Data Statistics:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2ee82-c732-4368-9dd4-167a0bca1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Results_Combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e2ba2e-c9a3-40ce-9907-ab26e42ea29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_Combine.to_csv(\"Results_Combine.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
